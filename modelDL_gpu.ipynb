{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0689cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가상환경 py37에서 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94e9fdb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdkit in c:\\users\\user-pc\\miniconda3\\envs\\py37\\lib\\site-packages (2023.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\user-pc\\miniconda3\\envs\\py37\\lib\\site-packages (from rdkit) (1.21.5)\n",
      "Requirement already satisfied: Pillow in c:\\users\\user-pc\\miniconda3\\envs\\py37\\lib\\site-packages (from rdkit) (9.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26f3e704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import umap\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import PandasTools, AllChem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f47a8ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12de1249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3090\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# GPU 이름 체크(cuda:0에 연결된 그래픽 카드 기준)\n",
    "print(torch.cuda.get_device_name()) # 'N\n",
    "# 사용 가능 GPU 개수 체크\n",
    "print(torch.cuda.device_count()) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ac62835",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc906e49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eb0849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(42) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae82adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef443c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "PandasTools.AddMoleculeColumnToFrame(train,'SMILES','Molecule')\n",
    "PandasTools.AddMoleculeColumnToFrame(test,'SMILES','Molecule')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b9661a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol2fp(mol):\n",
    "    fp = AllChem.GetHashedMorganFingerprint(mol, 6, nBits=4096)\n",
    "    ar = np.zeros((1,), dtype=np.int8)\n",
    "    DataStructs.ConvertToNumpyArray(fp, ar)\n",
    "    return ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93d4d250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FPs column 추가\n",
    "train[\"FPs\"] = train.Molecule.apply(mol2fp)\n",
    "test[\"FPs\"] = test.Molecule.apply(mol2fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5017145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 column만 추출\n",
    "train = train[['FPs','MLM', 'HLM']]\n",
    "test = test[['FPs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6294e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, target, transform, is_test=False):\n",
    "        self.df = df\n",
    "        self.target = target # HLM or MLM\n",
    "        self.is_test = is_test # train,valid / test\n",
    "\n",
    "        self.feature_select = transform\n",
    "        if not self.is_test: \n",
    "            self.fp = self.feature_select.fit_transform(np.stack(df['FPs']))\n",
    "        else: # valid or test\n",
    "            self.fp = self.feature_select.transform(np.stack(df['FPs']))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fp = self.fp[index]\n",
    "        if not self.is_test: # test가 아닌 경우(label 존재)\n",
    "            label = self.df[self.target][index]\n",
    "            return torch.tensor(fp).to('cuda').float(), torch.tensor(label).to('cuda').float().unsqueeze(dim=-1) # feature, label\n",
    "\n",
    "        else: # test인 경우\n",
    "            return torch.tensor(fp).to('cuda').float() # feature\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4867d91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting umap-learn\n",
      "  Using cached umap-learn-0.5.3.tar.gz (88 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user-pc\\miniconda3\\envs\\py37\\lib\\site-packages (from umap-learn) (1.21.5)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\users\\user-pc\\miniconda3\\envs\\py37\\lib\\site-packages (from umap-learn) (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\user-pc\\miniconda3\\envs\\py37\\lib\\site-packages (from umap-learn) (1.7.3)\n",
      "Collecting numba>=0.49\n",
      "  Downloading numba-0.56.4-cp37-cp37m-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 9.9 MB/s eta 0:00:00\n",
      "Collecting pynndescent>=0.5\n",
      "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 10.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: tqdm in c:\\users\\user-pc\\miniconda3\\envs\\py37\\lib\\site-packages (from umap-learn) (4.64.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\user-pc\\miniconda3\\envs\\py37\\lib\\site-packages (from numba>=0.49->umap-learn) (4.11.3)\n",
      "Collecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Downloading llvmlite-0.39.1-cp37-cp37m-win_amd64.whl (23.2 MB)\n",
      "     --------------------------------------- 23.2/23.2 MB 10.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\user-pc\\miniconda3\\envs\\py37\\lib\\site-packages (from numba>=0.49->umap-learn) (63.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\user-pc\\miniconda3\\envs\\py37\\lib\\site-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user-pc\\miniconda3\\envs\\py37\\lib\\site-packages (from scikit-learn>=0.22->umap-learn) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user-pc\\miniconda3\\envs\\py37\\lib\\site-packages (from tqdm->umap-learn) (0.4.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user-pc\\miniconda3\\envs\\py37\\lib\\site-packages (from importlib-metadata->numba>=0.49->umap-learn) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\user-pc\\miniconda3\\envs\\py37\\lib\\site-packages (from importlib-metadata->numba>=0.49->umap-learn) (4.3.0)\n",
      "Building wheels for collected packages: umap-learn, pynndescent\n",
      "  Building wheel for umap-learn (setup.py): started\n",
      "  Building wheel for umap-learn (setup.py): finished with status 'done'\n",
      "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82814 sha256=409b33033b790610a16390e552443670344ef82ee13aee82b7edc754aeb56f82\n",
      "  Stored in directory: c:\\users\\user-pc\\appdata\\local\\pip\\cache\\wheels\\b3\\52\\a5\\1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
      "  Building wheel for pynndescent (setup.py): started\n",
      "  Building wheel for pynndescent (setup.py): finished with status 'done'\n",
      "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55622 sha256=50675d8278cdf1d15ac890a9c13a40b6315d5f3ef8d1c785ecc49f3b3bc553ac\n",
      "  Stored in directory: c:\\users\\user-pc\\appdata\\local\\pip\\cache\\wheels\\e1\\38\\f7\\4d18bfa0426d0a999e6fb3ed8622298ea40b3ba807f903cd02\n",
      "Successfully built umap-learn pynndescent\n",
      "Installing collected packages: llvmlite, numba, pynndescent, umap-learn\n",
      "Successfully installed llvmlite-0.39.1 numba-0.56.4 pynndescent-0.5.10 umap-learn-0.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "229980a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'umap' has no attribute 'UMAP'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5768\\2677896944.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mumap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtransform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mumap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUMAP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_MLM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'MLM'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_HLM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'HLM'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'umap' has no attribute 'UMAP'"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "transform = umap.UMAP(n_components = 256)\n",
    "\n",
    "train_MLM = CustomDataset(df=train, target='MLM', transform=transform, is_test=False)\n",
    "train_HLM = CustomDataset(df=train, target='HLM', transform=transform, is_test=False)\n",
    "\n",
    "input_size = train_MLM.fp.shape[1]\n",
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e15272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "CFG = {'BATCH_SIZE': 256,\n",
    "       'EPOCHS': 5000,\n",
    "       'INPUT_SIZE': input_size,\n",
    "       'HIDDEN_SIZE': 1024,\n",
    "       'OUTPUT_SIZE': 1,\n",
    "       'DROPOUT_RATE': 0.5,\n",
    "       'LEARNING_RATE': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6966bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train,valid split\n",
    "train_MLM_dataset, valid_MLM_dataset = train_test_split(train_MLM, test_size=0.2, random_state=42)\n",
    "train_HLM_dataset, valid_HLM_dataset = train_test_split(train_HLM, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a721cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MLM_loader = DataLoader(dataset=train_MLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=True)\n",
    "\n",
    "valid_MLM_loader = DataLoader(dataset=valid_MLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)\n",
    "\n",
    "\n",
    "train_HLM_loader = DataLoader(dataset=train_HLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=True)\n",
    "\n",
    "valid_HLM_loader = DataLoader(dataset=valid_HLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_rate, out_size):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # fc 레이어 3개와 출력 레이어\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, out_size)\n",
    "        \n",
    "        # 정규화\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.ln3 = nn.LayerNorm(hidden_size)        \n",
    "        \n",
    "        # 활성화 함수\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "     \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x).to(device)\n",
    "        out = self.ln1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.ln2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        out = self.ln3(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d2fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLM = Net(CFG['INPUT_SIZE'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE']).to(device)\n",
    "model_HLM = Net(CFG['INPUT_SIZE'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5333888",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer_MLM = torch.optim.Adam(model_MLM.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "optimizer_HLM = torch.optim.Adam(model_HLM.parameters(), lr=CFG['LEARNING_RATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ac49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3a73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader, model, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        running_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            valid_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in valid_loader:\n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, targets)\n",
    "                    valid_loss += loss.item()\n",
    "                    \n",
    "            print(f'Epoch: {epoch}/{epochs}, Train Loss: {running_loss/len(train_loader)}, Valid Loss: {valid_loss/len(valid_HLM_loader)}')\n",
    "            \n",
    "            model.train()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdf4229",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Start: MLM\")\n",
    "model_MLM = train(train_MLM_loader, valid_MLM_loader, model_MLM, criterion, optimizer_MLM, epochs=CFG['EPOCHS'])\n",
    "\n",
    "print(\"Training Start: HLM\")\n",
    "model_HLM = train(train_HLM_loader, valid_HLM_loader, model_HLM, criterion, optimizer_HLM, epochs=CFG['EPOCHS'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96adf687",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_MLM = CustomDataset(df=test, target=None, transform=transform, is_test=True)\n",
    "test_HLM = CustomDataset(df=test, target=None, transform=transform, is_test=True)\n",
    "\n",
    "test_MLM_loader = DataLoader(dataset=test_MLM,\n",
    "                             batch_size=CFG['BATCH_SIZE'],\n",
    "                             shuffle=False)\n",
    "\n",
    "test_HLM_loader = DataLoader(dataset=test_HLM,\n",
    "                             batch_size=CFG['BATCH_SIZE'],\n",
    "                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db324d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(test_loader, model):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs in test_loader:\n",
    "            output = model(inputs)\n",
    "            preds.extend(output.cpu().numpy().flatten().tolist())\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126531ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_MLM = inference(test_MLM_loader, model_MLM)\n",
    "predictions_HLM = inference(test_HLM_loader, model_HLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a86133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41c6d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['MLM'] = predictions_MLM\n",
    "submission['HLM'] = predictions_HLM\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6523a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('data/baseline_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e88272",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e714e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost를 활용해보쟈\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145c16cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add22569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 column만 추출\n",
    "train_X = train[[ 'AlogP',\n",
    " 'Molecular_Weight',\n",
    " 'Num_H_Acceptors',\n",
    " 'Num_H_Donors',\n",
    " 'Num_RotatableBonds',\n",
    " 'LogD',\n",
    " 'Molecular_PolarSurfaceArea']]\n",
    "\n",
    "train_MLM = train['MLM']\n",
    "\n",
    "train_HLM = train['HLM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a332e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test[[ 'AlogP',\n",
    " 'Molecular_Weight',\n",
    " 'Num_H_Acceptors',\n",
    " 'Num_H_Donors',\n",
    " 'Num_RotatableBonds',\n",
    " 'LogD',\n",
    " 'Molecular_PolarSurfaceArea']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d008e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgboost.XGBRegressor(tree_method='gpu_hist', gpu_id=0,n_estimators = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e67f7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(train_X,train_MLM)\n",
    "predictionsMLM = xgb_model.predict(test_X)\n",
    "\n",
    "xgb_model.fit(train_X,train_HLM)\n",
    "predictionsHLM = xgb_model.predict(test_X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
